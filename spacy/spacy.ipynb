{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy is a Python library supporting various text analysis pipelines, such as named entity recognition, part-of-speech tagging, entity linking, etc., on over 70+ languages using large language models. It also supports adding custom components to their pipelines, training new models, and has some useful built-in visualizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.tokens import Token, Doc, Span\n",
    "from spacy.language import Language\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Finding words, phrases, names, & concepts\n",
    "\n",
    "spaCy's core functionality lies in the processing pipeline, typically called `nlp`. This object can be used like a function to analyze text. \n",
    "\n",
    "In the below cell a blank pipeline is made, containing only the language specific rules/components like those used for tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucian/mambaforge/envs/spacy/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing text with this object yields a `Doc` object. `Token` objects represent the tokens in a `Doc`, which can be indexed or iterated upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token at index 0 (iterator) is: Hello\n",
      "Token at index 1 (iterator) is: world\n",
      "Token at index 2 (iterator) is: !\n",
      "Token at index 1 (index) is: world\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello world!')\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Token at index {token.i} (iterator) is: {token.text}')\n",
    "\n",
    "print(f'Token at index 1 (index) is: {doc[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Span` objects are slices of the `Doc`, however it's only a view of the `Doc` and doesn't actually contain any data itself. They can be created using normal Python slicing on a `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The span text from index 1:3 is: world!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "print(f'The span text from index 1:3 is: {span.text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tokens` have a number of useful attributes, such as:\n",
    "- i: index within the parent document\n",
    "- text: token text\n",
    "- is_alpha: bool indicating whether token consists of alphabetic characters\n",
    "- is_punct: bool indicating whether token is punctuation\n",
    "- like_num: bool indiciating whether token \"resembles\" a number\n",
    "\n",
    "Attributes such as these are lexical attributes, they don't depend at all on how the token is used (its context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0, Text:     Google, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  1, Text:         is, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  2, Text:    looking, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  3, Text:         at, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  4, Text:     buying, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  5, Text:          a, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  6, Text:     London, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  7, Text:      based, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  8, Text:    company, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  9, Text:        for, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index: 10, Text:          $, Is alphabetic:   0, Is punctuation:   0, Like number:   0\n",
      "Index: 11, Text:         20, Is alphabetic:   0, Is punctuation:   0, Like number:   1\n",
      "Index: 12, Text:    million, Is alphabetic:   1, Is punctuation:   0, Like number:   1\n",
      "Index: 13, Text:          ., Is alphabetic:   0, Is punctuation:   1, Like number:   0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Google is looking at buying a London based company for $20 million.')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Index: {token.i:2d}, Text: {token.text:>10}, Is alphabetic: {token.is_alpha:3}, Is punctuation: {token.is_punct:3}, Like number: {token.like_num:3}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Pipelines\n",
    "\n",
    "Pipelines contain trained modles to make predictions using context, e.g. POS tags and named entities. The `spacy download` command can be used to download a trained pipeline, which then makes it available to be used by the `spacy.load` method. \n",
    "\n",
    "A pipeline's package contains the necessary weights for its models, the vocabulary, meta information, and the configuration file used to train it.\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_core_web_sm pipeline can now be loaded!\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "clear_output()\n",
    "print('en_core_web_sm pipeline can now be loaded!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a trained pipeline can we predict context dependent attributes, attributes returning strings usually end with a underscore, those without return a integer ID value from the central `Vocab`. Some context dependent attributes include:\n",
    "- pos_: predicted part-of-speech\n",
    "- dep_: dependency label, relationship between two tokens\n",
    "- head: syntactic head token, parent token this one is attached to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text:        She, Token POS:  PRON, Token POS ID: 95, Token Dependency: nsubj, Token Head: ate\n",
      "Token text:        ate, Token POS:  VERB, Token POS ID: 100, Token Dependency: ROOT, Token Head: ate\n",
      "Token text:        the, Token POS:   DET, Token POS ID: 90, Token Dependency: det, Token Head: pizza\n",
      "Token text:      large, Token POS:   ADJ, Token POS ID: 84, Token Dependency: amod, Token Head: pizza\n",
      "Token text:      pizza, Token POS:  NOUN, Token POS ID: 92, Token Dependency: dobj, Token Head: ate\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('She ate the large pizza')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Token text: {token.text:>10}, Token POS: {token.pos_:>5}, Token POS ID: {token.pos}, Token Dependency: {token.dep_}, Token Head: {token.head}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.ents` attribute on a `Doc` object access the named entities predicted by the NER model, it returns a list of `Span` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:       U.K., Label:   GPE\n",
      "Entity: $1 billion, Label: MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Apply is looking at buying U.K. startup for $1 billion after receiving positive reviews on their iPhone X case.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text:>10}, Label: {ent.label_:>5}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `spacy.explain` method can be used to get definitions for most tags & labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "direct object\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('GPE'))\n",
    "print(spacy.explain('NNP'))\n",
    "print(spacy.explain('dobj'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based \n",
    "\n",
    "The matcher in spaCy operates on `Doc` and `Token` objects instead of only strings like regex, allowing matches on lexical attributes or predicted attributes as well as text. \n",
    "\n",
    "Matcher patterns are lists of dictionaries, with each dictionary describing one token. The keys of the dictionaries are the names of attributes.\n",
    "\n",
    "The matcher needs to be initialized with the shared vocabulary, and uses the `add` method to add a pattern given a unique ID & list of patterns to add. Calling it on a `Doc` object returns a list of tuples containing the match ID, start index, & end index.\n",
    "\n",
    "Common keys to use for patterns are:\n",
    "- TEXT\n",
    "- LOWER\n",
    "- LEMMA\n",
    "- POS\n",
    "- IS_DIGIT\n",
    "- IS_PUNCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match ID: 9528407286733565721, Matched Span: iPhone X\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(f'Match ID: {match_id}, Matched Span: {matched_span}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators let you define how often a token should be matched, they can be added using the 'OP' key.\n",
    "- ?: makes the pattern optional\n",
    "- !: negates the token, matched 0 times\n",
    "- +: matches a token 1+ times\n",
    "- *: matches a token 0+ times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match: bought a smartphone\n",
      "Match: buying apps\n"
     ]
    }
   ],
   "source": [
    "pattern = [{'LEMMA': 'buy'}, {'POS': 'DET', 'OP': '?'}, {'POS': 'NOUN'}]\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('BUYING', [pattern])\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    print(f'Match: {doc[start:end]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Large-scale data analysis with spaCy\n",
    "\n",
    "spaCy stores all shared data in a central vocabulary that functions like a bidirectional lookup table for data shared across multiple documents. All strings are hashed and stored in the string store (available as `nlp.vocab.strings`), internally spaCy only communicates using the hash IDs.\n",
    "\n",
    "The `Doc` object also exposes its vocab & strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String: coffee, Hash: 3197928453018144401\n",
      "Coffee document hash: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "nlp.vocab.strings.add('coffee')\n",
    "coffee_hash = nlp.vocab.strings['coffee']\n",
    "coffee_str = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "print(f'String: {coffee_str}, Hash: {coffee_hash}')\n",
    "\n",
    "doc = nlp('I love coffee')\n",
    "print(f'Coffee document hash: {doc.vocab.strings[\"coffee\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Lexemes` are context-independent entries in the `Vocab`, they are returned by looking up a string or hash ID. `Lexemes` expose attributes just like `Tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexeme Test: coffee, Lexeme Hash: 3197928453018144401, Example attribute (is_alpha): True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab['coffee']\n",
    "\n",
    "print(f'Lexeme Test: {lexeme.text}, Lexeme Hash: {lexeme.orth}, Example attribute (is_alpha): {lexeme.is_alpha}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures: Doc, Span, & Token\n",
    "\n",
    "`Docs` are one of the central data structures in spaCy, created automatically by calling `nlp` on some text or manually by providing a list of words & list of bools indicates which words have spaces afterward (in addition to the shared vocab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "\n",
    "doc = Doc(nlp.vocab, words = words, spaces = spaces)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spans` are slices of `Docs`, taking in the `Doc` it refers to, the starting index, & the ending index (exclusive). It can also be given a optional label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "span = Span(doc, 0, 2, label = 'GREETING')\n",
    "\n",
    "print(span)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is optimized to work with `Docs` & `Spans`, so it's usually best to convert them to text as late as possible and to use built-in attributes as much as possible, e.g. `token.i` for the token index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and semantic similarity\n",
    "\n",
    "`Doc`, `Token`, & `Span` object all have a `similarity` method that takes in another object and returns a floating point number between 0 - 1 indicating how similar they are. Objects of different types can be compared, e.g. `Doc` and `Token`\n",
    "\n",
    "Doing this requires a medium or large pipeline, the small one doesn't ship with word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_core_web_md pipeline can now be loaded!\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "clear_output()\n",
    "\n",
    "print('en_core_web_md pipeline can now be loaded!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 <-> doc2 similarity: 0.869833325851152\n",
      "Token 1: pizza, Token 2: pasta, Similarity: 0.6850197911262512\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc1 = nlp('I like fast food')\n",
    "doc2 = nlp('I like pizza')\n",
    "print(f'doc1 <-> doc2 similarity: {doc1.similarity(doc2)}')\n",
    "\n",
    "doc = nlp('I like pizza and pasta')\n",
    "token1, token2 = doc[2], doc[4]\n",
    "print(f'Token 1: {token1}, Token 2: {token2}, Similarity: {token1.similarity(token2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity is measured using the cosine similarity between two vectors, objects composed of several tokens like `Doc` use the average of their token vectors.\n",
    "\n",
    "These vectors can also be accessed individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana: [ 0.20778  -2.4151    0.36605   2.0139   -0.23752  -3.1952   -0.2952\n",
      "  1.2272   -3.4129   -0.54969   0.32634  -1.0813    0.55626   1.5195\n",
      "  0.97797  -3.1816   -0.37207  -0.86093   2.1509   -4.0845    0.035405\n",
      "  3.5702   -0.79413  -1.7025   -1.6371   -3.198    -1.9387    0.91166\n",
      "  0.85409   1.8039   -1.103    -2.5274    1.6365   -0.82082   1.0278\n",
      " -1.705     1.5511   -0.95633  -1.4702   -1.865    -0.19324  -0.49123\n",
      "  2.2361    2.2119    3.6654    1.7943   -0.20601   1.5483   -1.3964\n",
      " -0.50819   2.1288   -2.332     1.3539   -2.1917    1.8923    0.28472\n",
      "  0.54285   1.2309    0.26027   1.9542    1.1739   -0.40348   3.2028\n",
      "  0.75381  -2.7179   -1.3587   -1.1965   -2.0923    2.2855   -0.3058\n",
      " -0.63174   0.70083   0.16899   1.2325    0.97006  -0.23356  -2.094\n",
      " -1.737     3.6075   -1.511    -0.9135    0.53878   0.49268   0.44751\n",
      "  0.6315    1.4963    4.1725    2.1961   -1.2409    0.4214    2.9678\n",
      "  1.841     3.0133   -4.4652    0.96521  -0.29787   4.3386   -1.2527\n",
      " -1.7734   -3.5637   -0.20035  -3.3013    0.99951  -0.92888  -0.94594\n",
      "  1.5124   -3.9385    2.7935   -3.1042    3.3382    0.54513  -0.37663\n",
      "  2.5151    0.51468  -0.88907   1.011     3.4705   -3.6037    1.3702\n",
      "  2.3468    1.6674    1.3904   -2.8112    2.237    -1.0344   -0.57164\n",
      "  1.0641   -1.6919    1.958    -0.78305   0.14741   0.51083   1.8278\n",
      " -0.69638   0.90548   0.62282  -1.8315   -2.8587    0.48424  -2.0527\n",
      " -0.53808  -2.3472    1.0354   -1.8257   -0.3892   -0.24943   0.8651\n",
      " -1.5195    1.2166   -2.698    -0.96698   2.2175   -0.16089  -0.49677\n",
      " -0.19646   1.3284    4.0824    1.3919    0.80669  -1.0316   -0.28056\n",
      " -1.8632    0.47716  -0.53628   1.3853   -2.1755   -0.2354    2.4933\n",
      " -0.87255   1.4493   -0.10778  -0.44159   1.3462    4.4211   -1.8385\n",
      "  0.3985    0.47637  -0.60074   3.3583   -0.15006  -0.40495   2.7225\n",
      " -1.6297    0.86797  -4.1445   -2.7793    1.1535   -0.011691  0.9792\n",
      " -1.0141    0.80134   0.43642   1.4337    2.8927    0.82871  -1.1827\n",
      " -1.3838    2.3903   -0.89323   1.1461   -1.7435    0.8654   -0.27075\n",
      " -0.78698   1.5631   -0.5923    0.098082 -0.26682   1.6282   -0.77495\n",
      "  3.2552    1.7964   -1.4314    1.2336    2.3102   -1.6328    2.8366\n",
      " -0.71384   0.43967   1.5627    3.079    -0.922    -0.43981  -0.7659\n",
      "  1.9362   -2.2479    1.041     0.63206   1.5855    3.4097   -2.9204\n",
      " -1.4751   -0.59534  -1.688    -4.1362    2.745    -2.8515    3.6509\n",
      " -0.66993  -2.8794    2.0733    1.1779   -2.0307    2.595    -0.12246\n",
      "  1.5844    1.1855    0.022385 -2.2916   -2.2684   -2.7537    0.34981\n",
      " -4.6243   -0.96521  -1.1435   -2.8894   -0.12619   2.9577   -1.7227\n",
      "  0.24757   1.2149    3.5349   -0.95802   0.080346 -1.6553   -0.6734\n",
      "  2.2918   -1.8229   -1.1336    1.8884    2.4789   -0.66061   2.0529\n",
      " -0.76687   0.32362  -2.2579    0.91278   0.36231   0.61562  -0.15396\n",
      " -0.42917  -0.89848   0.17298  -0.76978  -2.0222   -1.7127   -1.5632\n",
      "  0.56631  -1.354     2.6261    1.9156   -1.5651    1.8315   -1.4257\n",
      " -1.6861   -0.51953   1.7635   -0.50722   1.388    -1.1012  ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I have a banana.')\n",
    "print(f'{doc[3].text}: {doc[3].vector}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no objective measurement of similarity, it really just depends on the context and what the application needs to do. For example, the below cell has two pieces of text with opposing sentiments, yet are scored very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1 <-> doc2 similarity: 0.9534673962804006\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp('I love cats')\n",
    "doc2 = nlp('I hate cats')\n",
    "\n",
    "print(f'doc1 <-> doc2 similarity: {doc1.similarity(doc2)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining predictions & rules\n",
    "\n",
    "Statistical models are useful if you need to generalize from a few examples instead of listing out all possibilities, rule-based approaches might be more useful if there's a more finite number of instances we're searching for.\n",
    "\n",
    "Statistical components include the named-entity recognizerk, dependency parser, or part-of-speech tagger. Rule-based components include the tokenizer, `Matcher`, `PhraseMatcher`.\n",
    "\n",
    "The `PhraseMatcher` is more efficient & faster than the `Matcher`, taking in `Docs` and gives access to the tokens in context, better for large dictionaries & word lists on large volumes of text. It follows the same API as the base `Matcher`, except instead of a list of dictionaries it's provided a `Doc` object for the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp('Golden Retriever')\n",
    "matcher.add('DOG', [pattern])\n",
    "doc = nlp('I have a Golden Retriever')\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(f'Matched span: {span.text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Processing Pipelines\n",
    "\n",
    "Pipelines are a series of functions applied to a `Doc` to add attributes. The first thing a pipeline does is apply the tokenizer to turn the string into a `Doc` object, then the series of components are applied to the object in order. spaCy has several built-in components, such as:\n",
    "- tagger: part-of-speech tagger, creates `Token.tag` & `Token.pos`\n",
    "- parser: dependency parser, creates `Token.dep`, `Token.head`, `Doc.sents`, `Doc.noun_chunks`\n",
    "- ner: named entity recognizer, creates `Doc.ents`, `Token.ent_iob`, `Token.ent_type`\n",
    "- textcat: text classifier, creates `Doc.cats`\n",
    "\n",
    "All pipeline packages include several files & a `config.cfg`, which defines things like the language, which components to instantiate, & how they should be configured. The names of the pipeline components are stored in the `pipe_names` attribute, while the `pipeline` attribute stores the component names and function tuples.\n",
    "\n",
    "*Text categories are very specific, so it's not included in any pipelines by default, but it can be used to train your own.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component names: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Names and components: [('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f76709bb6b0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f7891573950>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f78915aaff0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f7890905210>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f789090cf50>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f78915ab8b0>)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Component names: {nlp.pipe_names}')\n",
    "print(f'Names and components: {nlp.pipeline}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom pipeline components\n",
    "\n",
    "Custom components allow you to change the behavior of the executed pipeline. A component is a function or callabale that takes a `Doc`, modifies it, & returns it to be processed by the next component. Custom components have to be decorated with `@Language.component` about the definition, providing a name. \n",
    "\n",
    "Once registered, it can be added using the `add_pipe` method providing the name given to the decorator. To specify where the component should be added, use the `first` or `last` keyword (which take a bool) or the `before` or `after` keywords (which take the string name of another component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pipeline: ['custom_component', 'tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Document Length: 3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "@Language.component('custom_component')\n",
    "def custom_component_function(doc):\n",
    "    print(f'Document Length: {len(doc)}')\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe('custom_component', first=True)\n",
    "# nlp.add_pipe('custom_component', after='ner')\n",
    "\n",
    "print(f'Current pipeline: {nlp.pipe_names}')\n",
    "\n",
    "doc = nlp('Hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('custom_component', <function __main__.custom_component_function(doc)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe('custom_component')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension attributes\n",
    "\n",
    "Custom attributes can add metadata to `Docs`, `Tokens`, & `Spans`, computed dynamically or added just once. These attributes are available via the `._` (dot underscore) property to be clear they were added by the user. They must be registered on the global class using the `set_extension` method, providing a attribute name and using keyword args to determine how the value should be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension('title', default = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of extensions:\n",
    "- attribute extensions: set a default value that can be overwritten\n",
    "- property extensions: define a getter & optional setter function, getter is called only when you retrieve the attribute & token only one argument (the token)\n",
    "- method extensions: make the attribute a callable method, taking in arguments & computing values dynamically, first argument is always the object itself\n",
    "  \n",
    "*If setting a extension attribute on `Span` you almost always want to use a property extension, otherwise you'd have to update every possible span ever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: The, is_color: False, is_title: True\n",
      "Token: sky, is_color: False, is_title: False\n",
      "Token: is, is_color: False, is_title: False\n",
      "Token: blue, is_color: True, is_title: False\n",
      "Token: ., is_color: False, is_title: False\n",
      "Document has token \"blue\"? True\n",
      "Document has token \"cloud\"? False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('The sky is blue.')\n",
    "\n",
    "\n",
    "#attribute\n",
    "Token.set_extension('is_color', default = False)\n",
    "doc[3]._.is_color = True\n",
    "\n",
    "#property\n",
    "def get_is_title(token):\n",
    "    return 'A' <= token.text[0] <= 'Z'\n",
    "Token.set_extension('is_title', getter = get_is_title)\n",
    "\n",
    "#method\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "Doc.set_extension('has_token', method = has_token)\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Token: {token.text}, is_color: {token._.is_color}, is_title: {token._.is_title}')\n",
    "\n",
    "print(f'Document has token \"blue\"? {doc._.has_token(\"blue\")}')\n",
    "print(f'Document has token \"cloud\"? {doc._.has_token(\"cloud\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Token.remove_extension('is_color')\n",
    "Token.remove_extension('is_title')\n",
    "Doc.remove_extension('has_token')\n",
    "Doc.remove_extension('title')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling & Performance\n",
    "\n",
    "The `nlp.pipe` method processes text as a stream, yielding `Doc` objects, making it much faster to process text batches. Since it's a generator, we need to wrap it in a list to get a list of `Docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOTS_OF_TEXT = ['The quick brown fox jumped over the lazy dog' for _ in range(100)]\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXT))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also supports context if you set `as_tuples` to True, yielding `Doc`/context tuples. This is useful for including extra metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the lazy dog 100\n",
      "The quick brown fox jumped over the lazy dog 74\n",
      "The quick brown fox jumped over the lazy dog 50\n",
      "The quick brown fox jumped over the lazy dog 91\n",
      "The quick brown fox jumped over the lazy dog 81\n",
      "The quick brown fox jumped over the lazy dog 93\n",
      "The quick brown fox jumped over the lazy dog 39\n",
      "The quick brown fox jumped over the lazy dog 8\n",
      "The quick brown fox jumped over the lazy dog 47\n",
      "The quick brown fox jumped over the lazy dog 68\n",
      "The quick brown fox jumped over the lazy dog 4\n",
      "The quick brown fox jumped over the lazy dog 22\n",
      "The quick brown fox jumped over the lazy dog 67\n",
      "The quick brown fox jumped over the lazy dog 60\n",
      "The quick brown fox jumped over the lazy dog 61\n",
      "The quick brown fox jumped over the lazy dog 71\n",
      "The quick brown fox jumped over the lazy dog 47\n",
      "The quick brown fox jumped over the lazy dog 42\n",
      "The quick brown fox jumped over the lazy dog 78\n",
      "The quick brown fox jumped over the lazy dog 9\n",
      "The quick brown fox jumped over the lazy dog 12\n",
      "The quick brown fox jumped over the lazy dog 16\n",
      "The quick brown fox jumped over the lazy dog 34\n",
      "The quick brown fox jumped over the lazy dog 77\n",
      "The quick brown fox jumped over the lazy dog 17\n",
      "The quick brown fox jumped over the lazy dog 12\n",
      "The quick brown fox jumped over the lazy dog 42\n",
      "The quick brown fox jumped over the lazy dog 85\n",
      "The quick brown fox jumped over the lazy dog 23\n",
      "The quick brown fox jumped over the lazy dog 18\n",
      "The quick brown fox jumped over the lazy dog 60\n",
      "The quick brown fox jumped over the lazy dog 50\n",
      "The quick brown fox jumped over the lazy dog 93\n",
      "The quick brown fox jumped over the lazy dog 69\n",
      "The quick brown fox jumped over the lazy dog 91\n",
      "The quick brown fox jumped over the lazy dog 97\n",
      "The quick brown fox jumped over the lazy dog 93\n",
      "The quick brown fox jumped over the lazy dog 18\n",
      "The quick brown fox jumped over the lazy dog 77\n",
      "The quick brown fox jumped over the lazy dog 59\n",
      "The quick brown fox jumped over the lazy dog 80\n",
      "The quick brown fox jumped over the lazy dog 85\n",
      "The quick brown fox jumped over the lazy dog 95\n",
      "The quick brown fox jumped over the lazy dog 97\n",
      "The quick brown fox jumped over the lazy dog 44\n",
      "The quick brown fox jumped over the lazy dog 31\n",
      "The quick brown fox jumped over the lazy dog 41\n",
      "The quick brown fox jumped over the lazy dog 6\n",
      "The quick brown fox jumped over the lazy dog 46\n",
      "The quick brown fox jumped over the lazy dog 4\n",
      "The quick brown fox jumped over the lazy dog 25\n",
      "The quick brown fox jumped over the lazy dog 62\n",
      "The quick brown fox jumped over the lazy dog 87\n",
      "The quick brown fox jumped over the lazy dog 65\n",
      "The quick brown fox jumped over the lazy dog 37\n",
      "The quick brown fox jumped over the lazy dog 64\n",
      "The quick brown fox jumped over the lazy dog 21\n",
      "The quick brown fox jumped over the lazy dog 66\n",
      "The quick brown fox jumped over the lazy dog 9\n",
      "The quick brown fox jumped over the lazy dog 36\n",
      "The quick brown fox jumped over the lazy dog 30\n",
      "The quick brown fox jumped over the lazy dog 58\n",
      "The quick brown fox jumped over the lazy dog 60\n",
      "The quick brown fox jumped over the lazy dog 30\n",
      "The quick brown fox jumped over the lazy dog 93\n",
      "The quick brown fox jumped over the lazy dog 80\n",
      "The quick brown fox jumped over the lazy dog 60\n",
      "The quick brown fox jumped over the lazy dog 70\n",
      "The quick brown fox jumped over the lazy dog 77\n",
      "The quick brown fox jumped over the lazy dog 8\n",
      "The quick brown fox jumped over the lazy dog 33\n",
      "The quick brown fox jumped over the lazy dog 39\n",
      "The quick brown fox jumped over the lazy dog 36\n",
      "The quick brown fox jumped over the lazy dog 1\n",
      "The quick brown fox jumped over the lazy dog 43\n",
      "The quick brown fox jumped over the lazy dog 94\n",
      "The quick brown fox jumped over the lazy dog 37\n",
      "The quick brown fox jumped over the lazy dog 69\n",
      "The quick brown fox jumped over the lazy dog 51\n",
      "The quick brown fox jumped over the lazy dog 13\n",
      "The quick brown fox jumped over the lazy dog 99\n",
      "The quick brown fox jumped over the lazy dog 84\n",
      "The quick brown fox jumped over the lazy dog 32\n",
      "The quick brown fox jumped over the lazy dog 86\n",
      "The quick brown fox jumped over the lazy dog 27\n",
      "The quick brown fox jumped over the lazy dog 59\n",
      "The quick brown fox jumped over the lazy dog 68\n",
      "The quick brown fox jumped over the lazy dog 71\n",
      "The quick brown fox jumped over the lazy dog 33\n",
      "The quick brown fox jumped over the lazy dog 51\n",
      "The quick brown fox jumped over the lazy dog 14\n",
      "The quick brown fox jumped over the lazy dog 12\n",
      "The quick brown fox jumped over the lazy dog 51\n",
      "The quick brown fox jumped over the lazy dog 42\n",
      "The quick brown fox jumped over the lazy dog 91\n",
      "The quick brown fox jumped over the lazy dog 55\n",
      "The quick brown fox jumped over the lazy dog 53\n",
      "The quick brown fox jumped over the lazy dog 57\n",
      "The quick brown fox jumped over the lazy dog 93\n",
      "The quick brown fox jumped over the lazy dog 55\n"
     ]
    }
   ],
   "source": [
    "LOTS_OF_TEXT = [(text, {'page_number': random.randint(1, 101)}) for text in LOTS_OF_TEXT]\n",
    "docs = list(nlp.pipe(LOTS_OF_TEXT, as_tuples=True))\n",
    "\n",
    "for doc, context in docs:\n",
    "    print(doc.text, context['page_number'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all you need is a tokenized `Doc`, and not to run the entire pipeline, the `nlp.make_doc` method can be used to tokenize some text and return a `Doc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp.make_doc('Hello world!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline components can also be temporarily disabled using the `nlp.select_pipes` context manager, taking in kwargs `enable` or `disable` with a list of string component names. Using this with a `with` block will automatically restore them upon exiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Google, London, $20 million)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucian/mambaforge/envs/spacy/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "with nlp.select_pipes(disable=['tagger', 'parser']):\n",
    "    doc = nlp('Google is looking at buying a London based company for $20 million.')\n",
    "    print(doc.ents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
