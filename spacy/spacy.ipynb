{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "spaCy is a Python library supporting various text analysis pipelines, such as named entity recognition, part-of-speech tagging, entity linking, etc., on over 70+ languages using large language models. It also supports adding custom components to their pipelines, training new models, and has some useful built-in visualizers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Finding words, phrases, names, & concepts\n",
    "\n",
    "spaCy's core functionality lies in the processing pipeline, typically called `nlp`. This object can be used like a function to analyze text. \n",
    "\n",
    "In the below cell a blank pipeline is made, containing only the language specific rules/components like those used for tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank('en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing text with this object yields a `Doc` object. `Token` objects represent the tokens in a `Doc`, which can be indexed or iterated upon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token at index 0 (iterator) is: Hello\n",
      "Token at index 1 (iterator) is: world\n",
      "Token at index 2 (iterator) is: !\n",
      "Token at index 1 (index) is: world\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Hello world!')\n",
    "\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Token at index {token.i} (iterator) is: {token.text}')\n",
    "\n",
    "print(f'Token at index 1 (index) is: {doc[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Span` objects are slices of the `Doc`, however it's only a view of the `Doc` and doesn't actually contain any data itself. They can be created using normal Python slicing on a `Doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The span text from index 1:3 is: world!\n"
     ]
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "print(f'The span text from index 1:3 is: {span.text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Token`s have a number of useful attributes, such as:\n",
    "- i: index within the parent document\n",
    "- text: token text\n",
    "- is_alpha: bool indicating whether token consists of alphabetic characters\n",
    "- is_punct: bool indicating whether token is punctuation\n",
    "- like_num: bool indiciating whether token \"resembles\" a number\n",
    "\n",
    "Attributes such as these are lexical attributes, they don't depend at all on how the token is used (its context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0, Text:     Google, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  1, Text:         is, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  2, Text:    looking, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  3, Text:         at, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  4, Text:     buying, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  5, Text:          a, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  6, Text:     London, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  7, Text:      based, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  8, Text:    company, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index:  9, Text:        for, Is alphabetic:   1, Is punctuation:   0, Like number:   0\n",
      "Index: 10, Text:          $, Is alphabetic:   0, Is punctuation:   0, Like number:   0\n",
      "Index: 11, Text:         20, Is alphabetic:   0, Is punctuation:   0, Like number:   1\n",
      "Index: 12, Text:    million, Is alphabetic:   1, Is punctuation:   0, Like number:   1\n",
      "Index: 13, Text:          ., Is alphabetic:   0, Is punctuation:   1, Like number:   0\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Google is looking at buying a London based company for $20 million.')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Index: {token.i:2d}, Text: {token.text:>10}, Is alphabetic: {token.is_alpha:3}, Is punctuation: {token.is_punct:3}, Like number: {token.like_num:3}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trained Pipelines\n",
    "\n",
    "Pipelines contain trained modles to make predictions using context, e.g. POS tags and named entities. The `spacy download` command can be used to download a trained pipeline, which then makes it available to be used by the `spacy.load` method. \n",
    "\n",
    "A pipeline's package contains the necessary weights for its models, the vocabulary, meta information, and the configuration file used to train it.\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_core_web_sm pipeline can now be loaded!\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "clear_output()\n",
    "print('en_core_web_sm pipeline can now be loaded!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a trained pipeline can we predict context dependent attributes, attributes returning strings usually end with a underscore, those without return a integer ID value from the central `Vocab`. Some context dependent attributes include:\n",
    "- pos_: predicted part-of-speech\n",
    "- dep_: dependency label, relationship between two tokens\n",
    "- head: syntactic head token, parent token this one is attached to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text:        She, Token POS:  PRON, Token POS ID: 95, Token Dependency: nsubj, Token Head: ate\n",
      "Token text:        ate, Token POS:  VERB, Token POS ID: 100, Token Dependency: ROOT, Token Head: ate\n",
      "Token text:        the, Token POS:   DET, Token POS ID: 90, Token Dependency: det, Token Head: pizza\n",
      "Token text:      large, Token POS:   ADJ, Token POS ID: 84, Token Dependency: amod, Token Head: pizza\n",
      "Token text:      pizza, Token POS:  NOUN, Token POS ID: 92, Token Dependency: dobj, Token Head: ate\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('She ate the large pizza')\n",
    "\n",
    "for token in doc:\n",
    "    print(f'Token text: {token.text:>10}, Token POS: {token.pos_:>5}, Token POS ID: {token.pos}, Token Dependency: {token.dep_}, Token Head: {token.head}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.ents` attribute on a `Doc` object access the named entities predicted by the NER model, it returns a list of `Span` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity:       U.K., Label:   GPE\n",
      "Entity: $1 billion, Label: MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Apply is looking at buying U.K. startup for $1 billion.')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text:>10}, Label: {ent.label_:>5}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
